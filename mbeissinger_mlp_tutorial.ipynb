{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yet Another MLP Example with Theano\n",
    "\n",
    "Adapted from: https://github.com/mbeissinger/intro_deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "(50000L, 784L) (50000L,)\n",
      "(10000L, 784L) (10000L,)\n",
      "(10000L, 784L) (10000L,)\n"
     ]
    }
   ],
   "source": [
    "# Download and unzip pickled version from here: http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = pickle.load(gzip.open('data/mnist.pkl.gz', 'rb'))\n",
    "print \"Shapes:\"\n",
    "print train_x.shape, train_y.shape\n",
    "print valid_x.shape, valid_y.shape\n",
    "print test_x.shape, test_y.shape\n",
    "\n",
    "# print \"--------------\"\n",
    "# print \"Example input:\"\n",
    "# print train_x[0]\n",
    "# print \"Example label:\"\n",
    "# print train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAAcCAAAAABkYnfcAAAQtklEQVR4nO1beVxV5dZ+EgQHUJFS\nLBw+FUcKmhyupab3NthgJlamxrW+zMzUrlfDm+ZshqalmUJq95opNpioOXyCQ0maESY4oCiiIIMT\ng4Cw13rPvn+cI3DOXu9RKX99f/D8tfdae717nX2e/b5reDdQgxrUoAY1qEENbgRBpzL+bBdqUIM/\nFIvy6Ls/2wdn/E9Mefs/24cbxpr0rtW0vG8lr7z3D/Xl9+AjW3KLW3qDuHhXSaeJaWsmTvT642/V\nZC/TwcZ//Li/A90LTy5oVE3boPgAvbIX23pWc1w9EswTtWVNLT8/vynvr2/2pa30PUEdcomILrod\nu09OO53q3er9Fp+AVyeJLGp5kelRnVVQp5E2Zmb+Vvqpnj333sCdF5Quc5G8XmSapmn2uQHja6j/\nyksfRpd8Hx0dHT3jfu1VQbHE/+ytUa4tvOsmblgFQ79ILi/f08D9RfUOZLWU5P1KF9S9iXv5BFS9\neixN9tBdGZ5ocOIYrdotItT7sqJ5uWkK7jZvMyxqHTMzZ3zNhT/0sl7x4Fmmy7nUrSpNHn7W+Zp3\nNulIHX7F4IdvwG9ntPr0IBF9LKnqrdeSulPk6TPEREREK3yten91rul1b/1+aeEgF1HjXNM0TfOy\n9l2y4gNVCTo0qZV8VTciflEzQt1M9eqN368S/t/xxU2bitQR/SXN7rmn+cvqqI+galu89babudtM\nGlfl7CGiNpoLw+MMgw1DXmNbLCxXX+pv4nOOr74iau42zfW1LNLQS+yAMXTAgK5WatbtcZqYfh5I\nPKmKdMoqp4tuW3ZIF4a9Z8ik7rL4EPO45xd3EXTtl14hPp1MOeKgC7WkjiWia6Smv1j1/kqFaNys\nxC76P4tsZLGZYZrmh+4tW7Sflpe30n6cppQ6HxcXF7ckLlEp1U80CDrF/Ix2uHg1SasDgH9E/If5\nsEX8y4U5fkD7QpoiWgVHbduWzjxrvW2HEPZ479onTAbX4DfyLLOzUzMpv8ovGKghdcNe6VcMI/mw\nhtTDC375+xTWBvIeE5gzOkoaz9Wm6ToDAfA7wczMCd+XFsgjrrLzJDyO11SRpjmT+k71H41DfS8Y\nyS28rfLnc4jjDhHxWouqwdLLRHSsRVuiHsKIDXdqST2a6NzMWTNnxmlJbdOT+uFtfgDw4oXUe6zK\ng2ayaZqttcYA+i65rJj5qP2s9ROtWzviS5/TSrkGNHbMoI136gd8TvtQAfQctc4gIip3nZD/6vif\npqt00fAtZi75dyazGiJoI0vdhDzdfmIiopVVZTOJLlWEV/UTiKQXsf9mgw3DGBYukrp2ROmshriX\ntY/iIWZ+QtQsMkVSo/9no5gT66FTlGh23yXm+Lc5855nbFXpd8qZ1FvUZNmfHpmGMcwq9uhWSPG9\nPet/TzzeogwnIkoNhIbUzU4yvSuvYh6BgU0BwPcM0ddCQO6vVDfZTwDHuAcApNieFZRhSaZpmuJs\nYcdn+5VSBUuGC+/vYKVKH5BsEkrS2upHRKC6KqddAbvOnCkg/pmIiDJclE+k2mfOu1W2NOdOLeYV\nc29HSC7nCq56ZW/R++OfTDlLH19DR6vGoa8Q0eZGjpNQEkk9hJltzPxyOLPwx43g0QDeydbdt+Wv\nzNvrSJrXSjSkhi+ieLBuwJBLRBvr94u4HeCiygLI3cXOpE5QmsJKNHOcIA4n2uILDCHKuN2i3EyU\ntqY58JRMakxmojd1DgMAwoqIFgpyf6X0hr9SXwAhhXKQG3DINM2vdbaNo9SFA8+1ay6oai8tUSpU\nMnqGaY67VDDQpkZI8r7pRETUrnG73qeJXFno7cib2in1umAcqdIDgDbr1JU3BO3kIjdFrr30PYC2\n54uqrmS1JhPRtdi/2XEp/BhywShKyzWMogHiTO2XF+MBtEjTkjqZ+fJfJcXwMjNRQ2ogkuM1yUHQ\nas49ONB+zLS6Qv6OciJ1k2wVKNr7s3H+Eat4JtPHvgCOEgkhZbOp3e8AgFc1pMb1SP1CHBFJ01TD\ny2qBzmiGkXI7UG8N7fUUtEM+UKZpjtMZL+SP6suaR1YoVTZCmBTRcB7TBAAYM2+eaBqo1GhJvp2I\nit98AMASojR/jUueKWq+IO6Swv+u1yyWL4yTjH7YqhkNAHbQCABtz2c5xQkNjhElOaLzUBJI3Z8N\nYwfCDWM0JFJ7HE3xA/CDitTdVzHPcpX59Hhl6SXTfKuNltT14vlvosIrlvIfbeyYTZh+qFCsVO9U\nvW6VOtZQsm+ZyIaQrkzh0u/qAN5PF/M02SMAwHIdqW3Mbkj9UkopEf0irlexWlIH5pT2BLCMzgjK\nDkcN09TG1HWnnXr6GYm2AB40lFJXn5DeE5+dNu4KvD3uFLNNiic1pP5bIdEpe8IQSyQR144kkdRe\n0ZzZP51ZfF0eMjoA6NVJHjCOB3l3ioxPDnYWLyeiEAC1R4/eI5A6PJ+LdnRE+C+jPVB/r5XUL3BP\nAIONwiaa3/Ghjbdbnm7HVNPMjwrybKUlNVoXZHwu0aQbUWWJ2ZnUlTVb30GxV9VL4rivG7zVWi5t\nmEPfAWiznyimnsajtyImRfxMe6zVGrsnmpm65eTdu3czEV16TV7XtaQOPkELAIwvI2nFH3DVNE3T\nNBeJtrN5jYbSwDx7Ue/nycEW1ZPE6W0R8i1R4RH+SYg1NaTeRrSnDwA0GpzvOJLgdVRNleTzmVlx\nlLiyLj3khfCLqnSUOGIO7dtPNNBV/CoRjUT38VOJiCjFtaRy0uAIAG2aAEC8ldRbjngCTXOtk7ED\nn2SpJGuICu/g4BYAWpnm3zWGeDafeaI1K0ngKr01G/9YcbxShQG4J3T8wiUFRXkbC0gsx/TPN3YL\n798dRM3vmLi3gMl4SvSm7gMbmW3MmZpqg47UwaeulfQ2yIaIlQsKHuFs432TvAL2l68Q7caUmqY2\nprappzW3A7pvznPUqufd4azxGU2Z0xG0mnO/CO3JRyRS20RSP5cYby+2RxD9pi+7t1OqK+Df512X\nKu1bzKw2Bok25QNRO32gz/Mlj0naw8XEVGhNlr+wl4TtTS92qSeH5HGVs53WRFG9C/ju5U80lcQH\ns1iNlFUA4J9trtcqg7czL3FdA58sobGVZ0yLK46X8MWkpCRW5fkJC166yzO3XBqzJTOvFOQNs4mJ\n6MxZEnMDzwfPUlHmukKi7H/KHVAtqdMrHq5cAUKsypfEQ4g4lWifxiMATwwePKxAQ+r9fEbMY+xo\nfu/j0ayUUjud85bHiaagSSzlL/bqfDR/sWCpi6kdeKqUrkrJHgDAq/VwpQ4uT8pQBZ87KWp9pZTa\nKBt1Uv3R9VMAi3aL+q4DmT63ikPtfQFHfyDaSdf5pGFUngnhRx8VjEdPq+O6/udSpQ430+gAIMEc\no1c2HErs2nQIo3MVs7fXHN5eJReauGHDhg0bhttrHq+pNGnITw3DEPuMXc5z6gcdm+4iKRao/TTR\n5L/A7yAR0fNio9zGvE78ES3+dX/nzp07LyDSkHqcSOrnjdLs3iFxREzGWW0t+rZpZpq1sdylNvym\nckEHnRUA4KV9Sik1wUk2kQjYS9QT3YjETDFQKXf7C5joNUFcp8WAuQcOpCillJGe/t79rZzVXzEz\nx8oj9lEd4NMYQEeWLwhmEqZ4B6lTj82fYiF1vFGV1EKiGFxy5OzVApuOmWMNZnecRoI2pgYAlHGZ\nS488jCrq914zKEPbJI5RcwVpyEnD0NbAADxsk7IVzzlEmxri9gNcOu0boq2PhIZarmEiclMyRgMt\nqZ9TxcI6H39yOICOPxIT6fsd3qZ51DUSDUg8PwTwZ+7uxh0AHruUUs6NgDn8LUKyeCyCTvFY0ShQ\nKTcVv9k2ZuuUUWfuEaWUys8sVyrKWp5r9i/FBz7jffKQfZSjce6rIfWLNtaQOu+XFx1HFlKPqzhp\nn2ecsISjw3avefpMnCYpCUzh8k9klQMJStjY4cDd07cwJ7kU9sLoI8dRyGr6Rj9wjNjSyDOMHzV1\nLgDAo0zWBKDW+1TwRiM8sI+O9YbvY6sKqPLFqsAnJJehr2GQltTPqBLhXxkTCAAPX6ZBHTvqm8SR\npvlPV1l28ZsAZvI2N71lAMB8pZRzm2kOfYOQs7TiTN7+O+XH5JbUtbcwjbIWYrep0o0f9+lxF46p\nNGHUoawifIaynDigr81B6qfEIA3oz3HCJijfFbs+tefBoUSU0aiqLr5K7619KmcLzVrgozKxNwW0\nOcIszZd2BHXpcvedYafC7mi5Stpr0G5xFjOXf+8iHsQZ9oO3L7Gbjq2G1GwYur06jgsEUo+kwhf8\nHl93habYJ8UXN22yNt1Gi6T27Oeo4g0v1JIaR9QSjabBYj4uyf1j7ZWdZgVCSS+iWCmVqtI1LYuA\nKY61sdYOpcofctJ1I+r2ej4R5+p8DVRKGwvVfY14lVBGtZ0KBQCPuVdyBJ96XeYnvVueYHlXSMVM\n7blZLvN02JhjqX04IZSIyGku3snsmPXrf8t8QoxGe/F0zXj9mFnY7QAAXh0/LDbNsoumaeblm9Ya\nd9O3TzIz77fk8GFU9nFIYFhsBqevcbcVO8YmdMJX2sSeaCXEmTqbihOPEdG7cjHPgePMNtf/+6Et\nFAgAfkMuExX11lguLNBV3yIoW5wXV5vHerXF/YOTTHOetfY9fnVubt6mdrK3TX9T9vynyVyl1G/O\nyvsK7amVmCMCcJso+qwlektqmKlfPQB4b1Sl0qo8S+2E55t5SpNfBmSNBADPzw6LHZ0GGfQPna92\ntMokouVVd5f2uWAYO+PHPBa/c69RFCHv5Dx3VregD2WOayRqmq41TdM8l7U9MjIyMjLStWDa5JHD\nzMwJz1qfURgRZR0loh91r5IdMSrcIgvJ5NJ52vItALwhkTqJiCh2fBv3227XE7ErqQ8SLZo9e/bs\nA0y04zmd5cJ8Te+0xUljmqjonmCa6ZsLTVMdcRdMSVirVEgdoM57BUrZCl2Tvn5xTLR8rD4XrJ2i\nJXUHolRRcUyt2DA9/DD9FCppZ3K8ZxhfkHf5ABhVMtI39OXU3+StRcvoC62vDnTJIiKn3kPPCwYb\nhmGwsUOY+QDg/nKpnQ8AOM2sWRreNk1zU2/N5wF+X51gZv6hv9SAu+snIibK/Uh3UwdihE1ovQwW\nSyKVCLaxldQ+QxdENNH4WonHZVI7SkvnlulfpoVqgKw4LhWrAADz37B3Xtx/JSHhf5VSifHxiUop\nVXgzXxg4cEBpyhTto0kqbAPAjNKysrKvxDozsIxjdjHLzQEAwKgS5vzp8vPvW1yk3yZ7DQ/kVO3X\nAcCd09gwDCNrraZo5/1ruq4D1+m8LlBCq0Nbh2tUXb4+w8x8ZZZm2ICpxDTf3U4yAECMrTqkxnGq\n7udlLZKtpA5dbi8sJX1sbd5V4lypZtfuJNKwHfCeMOFL08y/76bdbPXltc8Eyj+Qto1fD9HK8l2Z\nHauJtBVqdxjLrC5MEzcQXBctLxZrn1AVDP51hiuZXk4xkodpQmNgJGv/r1eu8sQbda8S7zNzypyZ\n4r6Nm0C4MFM33X1dUodTnLvC3M3Ca8R5+nrEdb5sWfvbrf280RleL0SN37MnKuqF0GqZt/xJXpg7\nbaQl2q/n3KHRhKKd46rlCuosppjqWV4HR5L0oWbG+dBbcs9bCN+ttE638tTADebSyWpx+vfgDf7x\nFnx3DCCnWmvO/1/4LnLbQ6mBBn2kDbu3Fg9mTqvmV8A1qEENfgf+C/434fbpcuVtAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show example images - using tile_raster_images helper function from OpenDeep to get 28x28 image from 784 array.\n",
    "from utils import tile_raster_images\n",
    "from PIL import Image as pil_img\n",
    "\n",
    "input_images = train_x[:25]\n",
    "im = pil_img.fromarray(\n",
    "    tile_raster_images(input_images, \n",
    "                       img_shape=(28, 28), \n",
    "                       tile_shape=(1, 25),\n",
    "                       tile_spacing=(1, 1))\n",
    ")\n",
    "im.save(\"some_mnist_numbers.png\")\n",
    "Image(filename=\"some_mnist_numbers.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your basic Theano imports.\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "\n",
    "x = T.matrix('x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "hidden_nodes = 50\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.084818892968\n"
     ]
    }
   ],
   "source": [
    "# Compute the hidden layer from the input\n",
    "import numpy\n",
    "import numpy.random as rng\n",
    "\n",
    "i = numpy.sqrt(6. / (input_size+hidden_nodes))\n",
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# W_x = numpy.asarray(rng.normal(loc=0.0, scale=.05, size=(input_size, hidden_nodes)), dtype=theano.config.floatX)\n",
    "W_x = numpy.asarray(rng.uniform(low=-i, high=i, size=(input_size, hidden_nodes)), dtype=theano.config.floatX)\n",
    "b_h = numpy.zeros(shape=(hidden_nodes,), dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_x = theano.shared(W_x, name=\"W_x\")\n",
    "b_h = theano.shared(b_h, name=\"b_h\")\n",
    "\n",
    "h = T.tanh(\n",
    "    T.dot(x, W_x) + b_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the output class probabilities from the hidden layer\n",
    "i = numpy.sqrt(6. / (hidden_nodes+output_size))\n",
    "# W_h = numpy.asarray(rng.normal(loc=0.0, scale=.05, size=(hidden_nodes, output_size)), dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_h = numpy.asarray(rng.uniform(low=-i, high=i, size=(hidden_nodes, output_size)), dtype=theano.config.floatX)\n",
    "b_y = numpy.zeros(shape=(output_size,), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_h = theano.shared(W_h, name=\"W_h\")\n",
    "b_y = theano.shared(b_y, name=\"b_y\")\n",
    "\n",
    "y = T.nnet.softmax(\n",
    "    T.dot(h, W_h) + b_y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The actual predicted label\n",
    "y_hat = T.argmax(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find cost compared to correct labels\n",
    "correct_labels = T.ivector(\"labels\")\n",
    "\n",
    "log_likelihood = T.log(y)[T.arange(correct_labels.shape[0]), correct_labels]\n",
    "cost = -T.mean(log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute gradient updates for the parameters\n",
    "parameters = [W_x, b_h, W_h, b_y]\n",
    "gradients = T.grad(cost, parameters)\n",
    "\n",
    "learning_rate = 0.01\n",
    "train_updates = [(param, param - learning_rate*gradient) for param, gradient in zip(parameters, gradients)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile function for training (changes parameters via updates) and testing (no updates)\n",
    "f_train = theano.function(\n",
    "    inputs=[x, correct_labels], \n",
    "    outputs=cost, \n",
    "    updates=train_updates, \n",
    "    allow_input_downcast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_test = theano.function(\n",
    "    inputs=[x], \n",
    "    outputs=y_hat, \n",
    "    allow_input_downcast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(batch_size=100, epochs=100, check_frequency=3):\n",
    "    # Main training loop\n",
    "    train_batches = len(train_x) / batch_size\n",
    "    valid_batches = len(valid_x) / batch_size\n",
    "    test_batches = len(test_x) / batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print epoch+1, \":\",\n",
    "\n",
    "        train_costs = []\n",
    "        train_accuracy = []\n",
    "        for i in range(train_batches):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_labels = train_y[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "            costs = f_train(batch_x, batch_labels)\n",
    "            preds = f_test(batch_x)\n",
    "            acc = sum(preds==batch_labels)/float(len(batch_labels))\n",
    "\n",
    "            train_costs.append(costs)\n",
    "            train_accuracy.append(acc)\n",
    "        print \"cost:\", numpy.mean(train_costs), \"\\ttrain:\", str(numpy.mean(train_accuracy)*100)+\"%\",\n",
    "\n",
    "        valid_accuracy = []\n",
    "        for i in range(valid_batches):\n",
    "            batch_x = valid_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_labels = valid_y[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "            preds = f_test(batch_x)\n",
    "            acc = sum(preds==batch_labels)/float(len(batch_labels))\n",
    "\n",
    "            valid_accuracy.append(acc)\n",
    "        print \"\\tvalid:\", str(numpy.mean(valid_accuracy)*100)+\"%\",\n",
    "\n",
    "        test_accuracy = []\n",
    "        for i in range(test_batches):\n",
    "            batch_x = test_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_labels = test_y[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "            preds = f_test(batch_x)\n",
    "            acc = sum(preds==batch_labels)/float(len(batch_labels))\n",
    "\n",
    "            test_accuracy.append(acc)\n",
    "        print \"\\ttest:\", str(numpy.mean(test_accuracy)*100)+\"%\"\n",
    "\n",
    "        if (epoch+1) % check_frequency == 0:\n",
    "            print 'saving filters...'\n",
    "            weight_filters = pil_img.fromarray(\n",
    "                    tile_raster_images(\n",
    "                        W_x.get_value(borrow=True).T,\n",
    "                        img_shape=(28, 28),\n",
    "#                         tile_shape=(20, 25),\n",
    "                        tile_shape=(7, 8),\n",
    "                        tile_spacing=(1, 1)\n",
    "                    )\n",
    "                )\n",
    "            weight_filters.save(\"mlp_filters_%d.png\"%(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : cost: 1.10856 \ttrain: 74.018% \tvalid: 85.67% \ttest: 84.98%\n",
      "2 : cost: 0.599191 \ttrain: 85.916% \tvalid: 88.45% \ttest: 87.97%\n",
      "3 : cost: 0.484731 \ttrain: 88.024% \tvalid: 89.6% \ttest: 89.19%\n",
      "saving filters...\n",
      "4 : cost: 0.428333 \ttrain: 88.974% \tvalid: 90.24% \ttest: 89.85%\n",
      "5 : cost: 0.393701 \ttrain: 89.63% \tvalid: 90.76% \ttest: 90.24%\n",
      "6 : cost: 0.369684 \ttrain: 90.106% \tvalid: 91.1% \ttest: 90.75%\n",
      "saving filters...\n",
      "7 : cost: 0.351661 \ttrain: 90.492% \tvalid: 91.44% \ttest: 91.01%\n",
      "8 : cost: 0.337367 \ttrain: 90.78% \tvalid: 91.65% \ttest: 91.23%\n",
      "9 : cost: 0.325559 \ttrain: 91.062% \tvalid: 91.87% \ttest: 91.46%\n",
      "saving filters...\n",
      "10 : cost: 0.315498 \ttrain: 91.276% \tvalid: 92.04% \ttest: 91.62%\n",
      "11 : cost: 0.306714 \ttrain: 91.506% \tvalid: 92.15% \ttest: 91.74%\n",
      "12 : cost: 0.298894 \ttrain: 91.714% \tvalid: 92.29% \ttest: 91.89%\n",
      "saving filters...\n",
      "13 : cost: 0.291826 \ttrain: 91.926% \tvalid: 92.4% \ttest: 92.1%\n",
      "14 : cost: 0.285356 \ttrain: 92.072% \tvalid: 92.53% \ttest: 92.3%\n",
      "15 : cost: 0.279375 \ttrain: 92.262% \tvalid: 92.68% \ttest: 92.47%\n",
      "saving filters...\n",
      "16 : cost: 0.2738 \ttrain: 92.408% \tvalid: 92.81% \ttest: 92.52%\n",
      "17 : cost: 0.268569 \ttrain: 92.532% \tvalid: 92.95% \ttest: 92.61%\n",
      "18 : cost: 0.263634 \ttrain: 92.654% \tvalid: 93.04% \ttest: 92.7%\n",
      "saving filters...\n",
      "19 : cost: 0.258958 \ttrain: 92.786% \tvalid: 93.24% \ttest: 92.77%\n",
      "20 : cost: 0.25451 \ttrain: 92.9% \tvalid: 93.33% \ttest: 92.87%\n",
      "21 : cost: 0.250266 \ttrain: 93.026% \tvalid: 93.39% \ttest: 92.99%\n",
      "saving filters...\n",
      "22 : cost: 0.246206 \ttrain: 93.136% \tvalid: 93.42% \ttest: 93.13%\n",
      "23 : cost: 0.242313 \ttrain: 93.248% \tvalid: 93.51% \ttest: 93.24%\n",
      "24 : cost: 0.238574 \ttrain: 93.364% \tvalid: 93.61% \ttest: 93.37%\n",
      "saving filters...\n",
      "25 : cost: 0.234975 \ttrain: 93.478% \tvalid: 93.68% \ttest: 93.46%\n",
      "26 : cost: 0.231508 \ttrain: 93.58% \tvalid: 93.83% \ttest: 93.55%\n",
      "27 : cost: 0.228163 \ttrain: 93.674% \tvalid: 93.94% \ttest: 93.67%\n",
      "saving filters...\n",
      "28 : cost: 0.224933 \ttrain: 93.774% \tvalid: 94.02% \ttest: 93.76%\n",
      "29 : cost: 0.221811 \ttrain: 93.854% \tvalid: 94.08% \ttest: 93.84%\n",
      "30 : cost: 0.218791 \ttrain: 93.938% \tvalid: 94.17% \ttest: 93.9%\n",
      "saving filters...\n",
      "31 : cost: 0.215868 \ttrain: 94.032% \tvalid: 94.28% \ttest: 93.93%\n",
      "32 : cost: 0.213036 \ttrain: 94.1% \tvalid: 94.36% \ttest: 94.01%\n",
      "33 : cost: 0.210291 \ttrain: 94.17% \tvalid: 94.4% \ttest: 94.06%\n",
      "saving filters...\n",
      "34 : cost: 0.207629 \ttrain: 94.226% \tvalid: 94.48% \ttest: 94.13%\n",
      "35 : cost: 0.205047 \ttrain: 94.298% \tvalid: 94.54% \ttest: 94.2%\n",
      "36 : cost: 0.20254 \ttrain: 94.356% \tvalid: 94.63% \ttest: 94.28%\n",
      "saving filters...\n",
      "37 : cost: 0.200105 \ttrain: 94.418% \tvalid: 94.64% \ttest: 94.38%\n",
      "38 : cost: 0.197738 \ttrain: 94.5% \tvalid: 94.65% \ttest: 94.41%\n",
      "39 : cost: 0.195438 \ttrain: 94.572% \tvalid: 94.73% \ttest: 94.39%\n",
      "saving filters...\n",
      "40 : cost: 0.1932 \ttrain: 94.634% \tvalid: 94.79% \ttest: 94.43%\n",
      "41 : cost: 0.191023 \ttrain: 94.674% \tvalid: 94.86% \ttest: 94.48%\n",
      "42 : cost: 0.188903 \ttrain: 94.728% \tvalid: 94.91% \ttest: 94.52%\n",
      "saving filters...\n",
      "43 : cost: 0.186838 \ttrain: 94.798% \tvalid: 94.97% \ttest: 94.53%\n",
      "44 : cost: 0.184825 \ttrain: 94.866% \tvalid: 95.02% \ttest: 94.54%\n",
      "45 : cost: 0.182863 \ttrain: 94.93% \tvalid: 95.08% \ttest: 94.59%\n",
      "saving filters...\n",
      "46 : cost: 0.18095 \ttrain: 94.982% \tvalid: 95.13% \ttest: 94.63%\n",
      "47 : cost: 0.179083 \ttrain: 95.028% \tvalid: 95.16% \ttest: 94.7%\n",
      "48 : cost: 0.17726 \ttrain: 95.068% \tvalid: 95.25% \ttest: 94.7%\n",
      "saving filters...\n",
      "49 : cost: 0.175481 \ttrain: 95.124% \tvalid: 95.28% \ttest: 94.79%\n",
      "50 : cost: 0.173742 \ttrain: 95.186% \tvalid: 95.33% \ttest: 94.79%\n",
      "51 : cost: 0.172043 \ttrain: 95.24% \tvalid: 95.39% \ttest: 94.79%\n",
      "saving filters...\n",
      "52 : cost: 0.170382 \ttrain: 95.294% \tvalid: 95.4% \ttest: 94.79%\n",
      "53 : cost: 0.168757 \ttrain: 95.36% \tvalid: 95.44% \ttest: 94.82%\n",
      "54 : cost: 0.167168 \ttrain: 95.414% \tvalid: 95.44% \ttest: 94.89%\n",
      "saving filters...\n",
      "55 : cost: 0.165613 \ttrain: 95.446% \tvalid: 95.46% \ttest: 94.9%\n",
      "56 : cost: 0.164091 \ttrain: 95.498% \tvalid: 95.46% \ttest: 94.95%\n",
      "57 : cost: 0.1626 \ttrain: 95.544% \tvalid: 95.46% \ttest: 95.03%\n",
      "saving filters...\n",
      "58 : cost: 0.16114 \ttrain: 95.592% \tvalid: 95.49% \ttest: 95.07%\n",
      "59 : cost: 0.159709 \ttrain: 95.646% \tvalid: 95.54% \ttest: 95.13%\n",
      "60 : cost: 0.158308 \ttrain: 95.676% \tvalid: 95.58% \ttest: 95.17%\n",
      "saving filters...\n",
      "61 : cost: 0.156933 \ttrain: 95.712% \tvalid: 95.6% \ttest: 95.21%\n",
      "62 : cost: 0.155586 \ttrain: 95.752% \tvalid: 95.61% \ttest: 95.22%\n",
      "63 : cost: 0.154264 \ttrain: 95.794% \tvalid: 95.63% \ttest: 95.24%\n",
      "saving filters...\n",
      "64 : cost: 0.152968 \ttrain: 95.82% \tvalid: 95.64% \ttest: 95.28%\n",
      "65 : cost: 0.151696 \ttrain: 95.88% \tvalid: 95.7% \ttest: 95.31%\n",
      "66 : cost: 0.150447 \ttrain: 95.904% \tvalid: 95.72% \ttest: 95.35%\n",
      "saving filters...\n",
      "67 : cost: 0.149222 \ttrain: 95.924% \tvalid: 95.77% \ttest: 95.36%\n",
      "68 : cost: 0.148018 \ttrain: 95.944% \tvalid: 95.79% \ttest: 95.38%\n",
      "69 : cost: 0.146836 \ttrain: 95.988% \tvalid: 95.82% \ttest: 95.42%\n",
      "saving filters...\n",
      "70 : cost: 0.145675 \ttrain: 96.024% \tvalid: 95.86% \ttest: 95.45%\n",
      "71 : cost: 0.144534 \ttrain: 96.052% \tvalid: 95.88% \ttest: 95.47%\n",
      "72 : cost: 0.143413 \ttrain: 96.072% \tvalid: 95.9% \ttest: 95.5%\n",
      "saving filters...\n",
      "73 : cost: 0.142311 \ttrain: 96.11% \tvalid: 95.92% \ttest: 95.51%\n",
      "74 : cost: 0.141227 \ttrain: 96.14% \tvalid: 95.93% \ttest: 95.53%\n",
      "75 : cost: 0.140161 \ttrain: 96.168% \tvalid: 95.95% \ttest: 95.55%\n",
      "saving filters...\n",
      "76 : cost: 0.139113 \ttrain: 96.182% \tvalid: 95.99% \ttest: 95.63%\n",
      "77 : cost: 0.138082 \ttrain: 96.212% \tvalid: 96.0% \ttest: 95.69%\n",
      "78 : cost: 0.137068 \ttrain: 96.258% \tvalid: 96.01% \ttest: 95.69%\n",
      "saving filters...\n",
      "79 : cost: 0.13607 \ttrain: 96.284% \tvalid: 96.04% \ttest: 95.71%\n",
      "80 : cost: 0.135087 \ttrain: 96.316% \tvalid: 96.05% \ttest: 95.74%\n",
      "81 : cost: 0.13412 \ttrain: 96.34% \tvalid: 96.07% \ttest: 95.77%\n",
      "saving filters...\n",
      "82 : cost: 0.133168 \ttrain: 96.368% \tvalid: 96.06% \ttest: 95.78%\n",
      "83 : cost: 0.13223 \ttrain: 96.392% \tvalid: 96.06% \ttest: 95.81%\n",
      "84 : cost: 0.131306 \ttrain: 96.404% \tvalid: 96.05% \ttest: 95.82%\n",
      "saving filters...\n",
      "85 : cost: 0.130397 \ttrain: 96.428% \tvalid: 96.06% \ttest: 95.85%\n",
      "86 : cost: 0.1295 \ttrain: 96.458% \tvalid: 96.07% \ttest: 95.87%\n",
      "87 : cost: 0.128617 \ttrain: 96.47% \tvalid: 96.08% \ttest: 95.87%\n",
      "saving filters...\n",
      "88 : cost: 0.127746 \ttrain: 96.492% \tvalid: 96.1% \ttest: 95.88%\n",
      "89 : cost: 0.126888 \ttrain: 96.52% \tvalid: 96.11% \ttest: 95.9%\n",
      "90 : cost: 0.126042 \ttrain: 96.552% \tvalid: 96.12% \ttest: 95.89%\n",
      "saving filters...\n",
      "91 : cost: 0.125208 \ttrain: 96.586% \tvalid: 96.14% \ttest: 95.9%\n",
      "92 : cost: 0.124385 \ttrain: 96.608% \tvalid: 96.17% \ttest: 95.93%\n",
      "93 : cost: 0.123574 \ttrain: 96.642% \tvalid: 96.18% \ttest: 95.96%\n",
      "saving filters...\n",
      "94 : cost: 0.122774 \ttrain: 96.662% \tvalid: 96.17% \ttest: 95.97%\n",
      "95 : cost: 0.121985 \ttrain: 96.67% \tvalid: 96.18% \ttest: 95.98%\n",
      "96 : cost: 0.121206 \ttrain: 96.686% \tvalid: 96.18% \ttest: 96.0%\n",
      "saving filters...\n",
      "97 : cost: 0.120437 \ttrain: 96.702% \tvalid: 96.19% \ttest: 96.0%\n",
      "98 : cost: 0.119679 \ttrain: 96.73% \tvalid: 96.2% \ttest: 96.02%\n",
      "99 : cost: 0.11893 \ttrain: 96.766% \tvalid: 96.22% \ttest: 96.03%\n",
      "saving filters...\n",
      "100 : cost: 0.118191 \ttrain: 96.79% \tvalid: 96.23% \ttest: 96.05%\n",
      "101 : cost: 0.117462 \ttrain: 96.81% \tvalid: 96.26% \ttest: 96.03%\n",
      "102 : cost: 0.116742 \ttrain: 96.826% \tvalid: 96.26% \ttest: 96.06%\n",
      "saving filters...\n",
      "103 : cost: 0.116031 \ttrain: 96.844% \tvalid: 96.28% \ttest: 96.09%\n",
      "104 : cost: 0.115328 \ttrain: 96.874% \tvalid: 96.34% \ttest: 96.1%\n",
      "105 : cost: 0.114635 \ttrain: 96.898% \tvalid: 96.35% \ttest: 96.12%\n",
      "saving filters...\n",
      "106 : cost: 0.11395 \ttrain: 96.914% \tvalid: 96.35% \ttest: 96.11%\n",
      "107 : cost: 0.113273 \ttrain: 96.938% \tvalid: 96.36% \ttest: 96.13%\n",
      "108 : cost: 0.112605 \ttrain: 96.956% \tvalid: 96.37% \ttest: 96.14%\n",
      "saving filters...\n",
      "109 : cost: 0.111944 \ttrain: 96.976% \tvalid: 96.37% \ttest: 96.14%\n",
      "110 : cost: 0.111292 \ttrain: 96.994% \tvalid: 96.37% \ttest: 96.14%\n",
      "111 : cost: 0.110647 \ttrain: 97.02% \tvalid: 96.37% \ttest: 96.18%\n",
      "saving filters...\n",
      "112 : cost: 0.110009 \ttrain: 97.032% \tvalid: 96.37% \ttest: 96.19%\n",
      "113 : cost: 0.109379 \ttrain: 97.054% \tvalid: 96.4% \ttest: 96.18%\n",
      "114 : cost: 0.108757 \ttrain: 97.076% \tvalid: 96.4% \ttest: 96.21%\n",
      "saving filters...\n",
      "115 : cost: 0.108141 \ttrain: 97.096% \tvalid: 96.43% \ttest: 96.21%\n",
      "116 : cost: 0.107533 \ttrain: 97.104% \tvalid: 96.42% \ttest: 96.21%\n",
      "117 : cost: 0.106931 \ttrain: 97.124% \tvalid: 96.42% \ttest: 96.22%\n",
      "saving filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 : cost: 0.106336 \ttrain: 97.142% \tvalid: 96.43% \ttest: 96.21%\n",
      "119 : cost: 0.105748 \ttrain: 97.152% \tvalid: 96.43% \ttest: 96.21%\n",
      "120 : cost: 0.105167 \ttrain: 97.168% \tvalid: 96.43% \ttest: 96.22%\n",
      "saving filters...\n",
      "121 : cost: 0.104591 \ttrain: 97.184% \tvalid: 96.42% \ttest: 96.23%\n",
      "122 : cost: 0.104022 \ttrain: 97.19% \tvalid: 96.44% \ttest: 96.23%\n",
      "123 : cost: 0.10346 \ttrain: 97.214% \tvalid: 96.44% \ttest: 96.22%\n",
      "saving filters...\n",
      "124 : cost: 0.102903 \ttrain: 97.24% \tvalid: 96.44% \ttest: 96.25%\n",
      "125 : cost: 0.102352 \ttrain: 97.258% \tvalid: 96.49% \ttest: 96.28%\n",
      "126 : cost: 0.101807 \ttrain: 97.276% \tvalid: 96.5% \ttest: 96.29%\n",
      "saving filters...\n",
      "127 : cost: 0.101268 \ttrain: 97.286% \tvalid: 96.5% \ttest: 96.32%\n",
      "128 : cost: 0.100735 \ttrain: 97.302% \tvalid: 96.52% \ttest: 96.32%\n",
      "129 : cost: 0.100207 \ttrain: 97.326% \tvalid: 96.53% \ttest: 96.33%\n",
      "saving filters...\n",
      "130 : cost: 0.0996844 \ttrain: 97.354% \tvalid: 96.54% \ttest: 96.34%\n",
      "131 : cost: 0.0991674 \ttrain: 97.372% \tvalid: 96.53% \ttest: 96.36%\n",
      "132 : cost: 0.0986556 \ttrain: 97.384% \tvalid: 96.53% \ttest: 96.37%\n",
      "saving filters...\n",
      "133 : cost: 0.098149 \ttrain: 97.398% \tvalid: 96.54% \ttest: 96.38%\n",
      "134 : cost: 0.0976476 \ttrain: 97.418% \tvalid: 96.53% \ttest: 96.39%\n",
      "135 : cost: 0.0971512 \ttrain: 97.432% \tvalid: 96.51% \ttest: 96.41%\n",
      "saving filters...\n",
      "136 : cost: 0.0966597 \ttrain: 97.448% \tvalid: 96.52% \ttest: 96.43%\n",
      "137 : cost: 0.0961731 \ttrain: 97.464% \tvalid: 96.51% \ttest: 96.42%\n",
      "138 : cost: 0.0956912 \ttrain: 97.478% \tvalid: 96.51% \ttest: 96.42%\n",
      "saving filters...\n",
      "139 : cost: 0.0952141 \ttrain: 97.49% \tvalid: 96.52% \ttest: 96.41%\n",
      "140 : cost: 0.0947416 \ttrain: 97.494% \tvalid: 96.54% \ttest: 96.41%\n",
      "141 : cost: 0.0942736 \ttrain: 97.506% \tvalid: 96.53% \ttest: 96.4%\n",
      "saving filters...\n",
      "142 : cost: 0.0938101 \ttrain: 97.524% \tvalid: 96.54% \ttest: 96.4%\n",
      "143 : cost: 0.0933511 \ttrain: 97.538% \tvalid: 96.54% \ttest: 96.4%\n",
      "144 : cost: 0.0928963 \ttrain: 97.56% \tvalid: 96.53% \ttest: 96.4%\n",
      "saving filters...\n",
      "145 : cost: 0.0924459 \ttrain: 97.576% \tvalid: 96.54% \ttest: 96.45%\n",
      "146 : cost: 0.0919996 \ttrain: 97.584% \tvalid: 96.56% \ttest: 96.44%\n",
      "147 : cost: 0.0915575 \ttrain: 97.596% \tvalid: 96.56% \ttest: 96.44%\n",
      "saving filters...\n",
      "148 : cost: 0.0911194 \ttrain: 97.602% \tvalid: 96.56% \ttest: 96.45%\n",
      "149 : cost: 0.0906854 \ttrain: 97.622% \tvalid: 96.57% \ttest: 96.48%\n",
      "150 : cost: 0.0902553 \ttrain: 97.642% \tvalid: 96.57% \ttest: 96.48%\n",
      "saving filters...\n",
      "151 : cost: 0.0898291 \ttrain: 97.658% \tvalid: 96.58% \ttest: 96.48%\n",
      "152 : cost: 0.0894067 \ttrain: 97.676% \tvalid: 96.58% \ttest: 96.49%\n",
      "153 : cost: 0.0889881 \ttrain: 97.686% \tvalid: 96.6% \ttest: 96.49%\n",
      "saving filters...\n",
      "154 : cost: 0.0885732 \ttrain: 97.69% \tvalid: 96.61% \ttest: 96.5%\n",
      "155 : cost: 0.0881619 \ttrain: 97.704% \tvalid: 96.6% \ttest: 96.5%\n",
      "156 : cost: 0.0877543 \ttrain: 97.714% \tvalid: 96.59% \ttest: 96.5%\n",
      "saving filters...\n",
      "157 : cost: 0.0873502 \ttrain: 97.72% \tvalid: 96.6% \ttest: 96.52%\n",
      "158 : cost: 0.0869496 \ttrain: 97.742% \tvalid: 96.62% \ttest: 96.51%\n",
      "159 : cost: 0.0865524 \ttrain: 97.756% \tvalid: 96.63% \ttest: 96.51%\n",
      "saving filters...\n",
      "160 : cost: 0.0861586 \ttrain: 97.778% \tvalid: 96.63% \ttest: 96.51%\n",
      "161 : cost: 0.0857682 \ttrain: 97.786% \tvalid: 96.63% \ttest: 96.51%\n",
      "162 : cost: 0.0853811 \ttrain: 97.798% \tvalid: 96.63% \ttest: 96.53%\n",
      "saving filters...\n",
      "163 : cost: 0.0849972 \ttrain: 97.808% \tvalid: 96.62% \ttest: 96.55%\n",
      "164 : cost: 0.0846166 \ttrain: 97.824% \tvalid: 96.63% \ttest: 96.55%\n",
      "165 : cost: 0.084239 \ttrain: 97.842% \tvalid: 96.65% \ttest: 96.56%\n",
      "saving filters...\n",
      "166 : cost: 0.0838646 \ttrain: 97.852% \tvalid: 96.66% \ttest: 96.56%\n",
      "167 : cost: 0.0834933 \ttrain: 97.874% \tvalid: 96.66% \ttest: 96.57%\n",
      "168 : cost: 0.083125 \ttrain: 97.884% \tvalid: 96.67% \ttest: 96.58%\n",
      "saving filters...\n",
      "169 : cost: 0.0827597 \ttrain: 97.892% \tvalid: 96.67% \ttest: 96.58%\n",
      "170 : cost: 0.0823973 \ttrain: 97.9% \tvalid: 96.67% \ttest: 96.58%\n",
      "171 : cost: 0.0820378 \ttrain: 97.916% \tvalid: 96.68% \ttest: 96.58%\n",
      "saving filters...\n",
      "172 : cost: 0.0816812 \ttrain: 97.924% \tvalid: 96.7% \ttest: 96.58%\n",
      "173 : cost: 0.0813274 \ttrain: 97.928% \tvalid: 96.69% \ttest: 96.58%\n",
      "174 : cost: 0.0809764 \ttrain: 97.94% \tvalid: 96.71% \ttest: 96.59%\n",
      "saving filters...\n",
      "175 : cost: 0.0806281 \ttrain: 97.96% \tvalid: 96.72% \ttest: 96.6%\n",
      "176 : cost: 0.0802826 \ttrain: 97.968% \tvalid: 96.71% \ttest: 96.62%\n",
      "177 : cost: 0.0799397 \ttrain: 97.986% \tvalid: 96.72% \ttest: 96.63%\n",
      "saving filters...\n",
      "178 : cost: 0.0795995 \ttrain: 97.988% \tvalid: 96.72% \ttest: 96.65%\n",
      "179 : cost: 0.0792619 \ttrain: 97.996% \tvalid: 96.73% \ttest: 96.66%\n",
      "180 : cost: 0.0789268 \ttrain: 98.002% \tvalid: 96.72% \ttest: 96.68%\n",
      "saving filters...\n",
      "181 : cost: 0.0785943 \ttrain: 98.006% \tvalid: 96.72% \ttest: 96.68%\n",
      "182 : cost: 0.0782643 \ttrain: 98.012% \tvalid: 96.72% \ttest: 96.68%\n",
      "183 : cost: 0.0779367 \ttrain: 98.012% \tvalid: 96.72% \ttest: 96.7%\n",
      "saving filters...\n",
      "184 : cost: 0.0776116 \ttrain: 98.016% \tvalid: 96.73% \ttest: 96.71%\n",
      "185 : cost: 0.077289 \ttrain: 98.032% \tvalid: 96.74% \ttest: 96.73%\n",
      "186 : cost: 0.0769686 \ttrain: 98.038% \tvalid: 96.76% \ttest: 96.73%\n",
      "saving filters...\n",
      "187 : cost: 0.0766507 \ttrain: 98.04% \tvalid: 96.76% \ttest: 96.73%\n",
      "188 : cost: 0.0763351 \ttrain: 98.054% \tvalid: 96.78% \ttest: 96.73%\n",
      "189 : cost: 0.0760217 \ttrain: 98.064% \tvalid: 96.8% \ttest: 96.73%\n",
      "saving filters...\n",
      "190 : cost: 0.0757106 \ttrain: 98.066% \tvalid: 96.81% \ttest: 96.74%\n",
      "191 : cost: 0.0754018 \ttrain: 98.074% \tvalid: 96.82% \ttest: 96.76%\n",
      "192 : cost: 0.0750951 \ttrain: 98.082% \tvalid: 96.83% \ttest: 96.77%\n",
      "saving filters...\n",
      "193 : cost: 0.0747907 \ttrain: 98.094% \tvalid: 96.84% \ttest: 96.78%\n",
      "194 : cost: 0.0744884 \ttrain: 98.106% \tvalid: 96.84% \ttest: 96.78%\n",
      "195 : cost: 0.0741882 \ttrain: 98.12% \tvalid: 96.84% \ttest: 96.78%\n",
      "saving filters...\n",
      "196 : cost: 0.0738901 \ttrain: 98.126% \tvalid: 96.84% \ttest: 96.78%\n",
      "197 : cost: 0.0735941 \ttrain: 98.136% \tvalid: 96.84% \ttest: 96.79%\n",
      "198 : cost: 0.0733002 \ttrain: 98.144% \tvalid: 96.84% \ttest: 96.79%\n",
      "saving filters...\n",
      "199 : cost: 0.0730083 \ttrain: 98.152% \tvalid: 96.85% \ttest: 96.79%\n",
      "200 : cost: 0.0727184 \ttrain: 98.156% \tvalid: 96.84% \ttest: 96.8%\n"
     ]
    }
   ],
   "source": [
    "run(batch_size=100, epochs=200, check_frequency=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
